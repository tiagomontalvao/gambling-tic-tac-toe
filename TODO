BUGS:

TODO
- Retrain v4 and v5 changing drl_player to consider current coins, not sum_cois
- Try to change model to have a Conv2d layer
- Investigate why Agent bids all its money
- Experiment with different network sizes
- Add tqdm progress bars to training and validation
- Implement prioritized experience replay buffer
- Try to change coin action of [0, 1] from total initial coins to total current coins. E.g: 0.5 from 150 coins should represent 50, not 75.
- Add to training loop
    X time
    - patience
- Add asserts
- Add docstrings
- Check TODOs

DOING:
- Analyse Critic values for first move
- Analyse performance without BatchNorm1d


DONE:
- Change state coins from (..., coins[0], coins[1]) to (..., coins[player], coins[opponent])
- Invalidate invalid movements
- Normalize bid in NN input to [0, 2*initial_value], so that all bids are transformed (bid/(2*initial_value)) and are mapped to the range [0,1).
- Create training loop for agent
- Add noise to agent.action() when choosing bids
- Add plot loss functions
- [BUG] Investigate why agent.memory.sample()[0] (state) returns coins in the order of 1e-5
    - Wrong implementation of utils functions
- [BUG] Investigate if last_state == curr_state can happen (it did during debug in jupyter notebook)
    - When both players bid the same value, the state does not change
- Add -0.05 to every move so that agent tries to finish the game earlier
- Remove equal bid moves from replay buffer
    - Equals bids now incur in randomly choosing a player to play, so there is no more problem with this 

============================================================================================================

Parte escrita:

Título:
APLICANDO MODELOS DE APRENDIZADO POR REFORÇO PROFUNDO EM UM JOGO ADVERSÁRIO

Capítulos:

1. INTRODUÇÃO
- Histórico da área
	- ML
	- DL
	- RL
		- Atari
- Problema abordado
- Objetivos do trabalho

2. CONCEITOS BÁSICOS
- ML: Supervised x Unsupervised x Reinforcement
- Cadeias de Markov
- Métricas de avaliação

3. MODELAGEM
- Markov Decision Processes
- Q-Learning
- Redes Neurais
- DQN
- DDPG
- Adaptação para o problema

4. EXPERIMENTOS
- GitHub
- Tecnologias
- Hiperparâmetros escolhidos
	- LR
	- Tamanho da rede
	- Rewards por movimento e final
	- Gamma
	- Batch Size
	- Gradient Clipping (http://proceedings.mlr.press/v28/pascanu13.pdf)
- Resultados obtidos
- Desafios
	- Implementação do modelo
		- Complexo
		- Bugs
		- Self play não convergia -> talvez deixar muito mais tempo treinando
	- Tempo para iterar o modelo
		- GPU
		- Duração de cada treinamento
		- Comentar quantas versões foram treinadas 
	- Tuning de hiperparâmetros
	- Natureza do problema
		- Diferença abrupta em pequenas variações do bid_value

5. CONCLUSÃO
- Lições aprendidas
- Trabalhos futuros
	- Poderia colocar lógicas fixas, mas achei melhor explorar o potencial do modelo estatístico
	- Explorar diferentes arquiteturas
		- entradas de coins e board diferentes vs iguais
		- profundidade da rede
		- largura da rede
		- usar camada convolucional para processar o board
